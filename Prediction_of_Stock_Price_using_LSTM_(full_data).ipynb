{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0I85DwHCG3vvat3/iAPVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akif4362/lstm_research/blob/main/Prediction_of_Stock_Price_using_LSTM_(full_data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing Libraries & Setting Up Device Agnostic Code"
      ],
      "metadata": {
        "id": "KnsRFNvvakrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from copy import deepcopy as dc\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "flFp_9ru6LwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Downloading and Preparing Dataset"
      ],
      "metadata": {
        "id": "5Lm3XWiGfIDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = Path(\"data/\")\n",
        "csv_path = data_path / \"stock_prices_data\"\n",
        "\n",
        "if csv_path.is_dir():\n",
        "    print(f\"{csv_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {csv_path} directory, creating one...\")\n",
        "    csv_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(data_path / \"timeseries_research_data.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/Akif4362/lstm_research/raw/main/data/timeseries_research_data.zip\")\n",
        "        print(\"Downloading time series data...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    with zipfile.ZipFile(data_path / \"timeseries_research_data.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping time series data...\")\n",
        "        zip_ref.extractall(csv_path)"
      ],
      "metadata": {
        "id": "yNzgGgFd9eiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/data/stock_prices_data/timeseries_research_data/AAMRATECH_data.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "eKJ7mFZSF24l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[[\"Date\", \"Close\"]]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "l6BS4ysHGDUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing datatype of Date column in DateTime Format\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "# Visualizing the time-series data\n",
        "plt.plot(df[\"Date\"], df[\"Close\"])\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Close')\n",
        "plt.title(f\"AAMRATECH\")\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "HggZPTXGGhvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare dataframe for creating input vectors for LSTM\n",
        "def prepare_dataframe_for_lstm(df, n_steps):\n",
        "  df = dc(df)\n",
        "\n",
        "  df.set_index(\"Date\", inplace=True)\n",
        "\n",
        "  for i in range(1, n_steps+1):\n",
        "    df[f\"Close(t-{i})\"] = df[\"Close\"].shift(i)\n",
        "\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "TIMESTEP = 7\n",
        "shifted_df = prepare_dataframe_for_lstm(df, TIMESTEP)\n",
        "shifted_df"
      ],
      "metadata": {
        "id": "OJ66i1NsGwN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting dataframe to numpy array\n",
        "shifted_df_as_np = shifted_df.to_numpy()\n",
        "shifted_df_as_np"
      ],
      "metadata": {
        "id": "-Jat54hELnjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling our data to be between -1 and 1\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "shifted_df_as_np = scaler.fit_transform(shifted_df_as_np)\n",
        "shifted_df_as_np"
      ],
      "metadata": {
        "id": "-G7EmNO1gxdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking our input and output\n",
        "X = shifted_df_as_np[:, 1:]\n",
        "y = shifted_df_as_np[:, 0]\n",
        "\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "Z_nnpUEMhTeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flipping the X array to match proper sequence of input for LSTM\n",
        "X = dc(np.flip(X, axis=1))\n",
        "X"
      ],
      "metadata": {
        "id": "PSimymQohy1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into train and test\n",
        "split_index = int(len(X) * 0.90)\n",
        "\n",
        "X_train = X[:split_index]\n",
        "X_test = X[split_index:]\n",
        "y_train = y[:split_index]\n",
        "y_test = y[split_index:]\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "qFRS4Skzh8eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixing the dimensions as required for LSTM\n",
        "X_train = X_train.reshape((-1, 7, 1))\n",
        "X_test = X_test.reshape((-1, 7, 1))\n",
        "\n",
        "y_train = y_train.reshape((-1, 1))\n",
        "y_test = y_test.reshape((-1, 1))\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "uXwPeZxKivC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting numpy array into PyTorch tensors\n",
        "X_train = torch.tensor(X_train).float()\n",
        "X_test = torch.tensor(X_test).float()\n",
        "y_train = torch.tensor(y_train).float()\n",
        "y_test = torch.tensor(y_test).float()\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "-tmkMwSJjGJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Creating PyTorch Dataset and Dataloader"
      ],
      "metadata": {
        "id": "6Pd4kxIpfTSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclassing from the Dataset class to get out train and test datasets\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TimeSeriesData(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    return self.X[i], self.y[i]\n",
        "\n",
        "train_dataset = TimeSeriesData(X_train, y_train)\n",
        "test_dataset = TimeSeriesData(X_test, y_test)"
      ],
      "metadata": {
        "id": "14FYf3BUj0fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "PRD0NntrlffH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _, (X, y) in enumerate(train_dataloader):\n",
        "  print(X.shape, y.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "CCjjzEEMmg28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Creating our LSTM Model"
      ],
      "metadata": {
        "id": "dlgO2qBSfakk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our LSTM model\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_stacked_layers):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_stacked_layers = num_stacked_layers\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
        "    c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
        "\n",
        "    out, _ = self.lstm(x, (h0, c0))\n",
        "    out = self.fc(out[:, -1, :])\n",
        "    return out\n",
        "\n",
        "model = LSTM(1, 5, 1)\n",
        "model.to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "MHJbXP_Omv77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Predicting Closing Price for AAMRATECH"
      ],
      "metadata": {
        "id": "bWVDF6lgflb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up our loss and optimizer\n",
        "loss_fn = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "0cLkh8t3pJYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating train step\n",
        "def train_step(model, dataloader, loss_fn, optimizer, epoch, print_every=40, device=device):\n",
        "  \"\"\"trains model for one epoch\"\"\"\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    y_pred = model(X)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "\n",
        "  if (epoch + 1) % print_every == 0:\n",
        "    print(f\"Epoch {epoch + 1}, train_loss: {train_loss:.5f}\")"
      ],
      "metadata": {
        "id": "0S5PEKghqmCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating test step\n",
        "def test_step(model, dataloader, loss_fn, epoch, print_every=40, device=device):\n",
        "  \"\"\"tests model for one epoch\"\"\"\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      y_logit = model(X)\n",
        "      loss = loss_fn(y_logit, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "\n",
        "  if (epoch + 1) % print_every == 0:\n",
        "    print(f\"Epoch {epoch + 1}, test_loss: {test_loss:.5f}\\n-------------------------------\")"
      ],
      "metadata": {
        "id": "4VG8ypzTthjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traing our model\n",
        "epochs = 200\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_step(model, train_dataloader, loss_fn, optimizer, epoch=epoch)\n",
        "  test_step(model, test_dataloader, loss_fn, epoch=epoch)"
      ],
      "metadata": {
        "id": "uHV-fAYUur_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the unscaled values of the predictions on test set\n",
        "test_predictions = model(X_test.to(device)).detach().cpu().numpy().flatten()\n",
        "\n",
        "dummies = np.zeros((X_test.shape[0], TIMESTEP+1))\n",
        "dummies[:, 0] = test_predictions\n",
        "dummies = scaler.inverse_transform(dummies)\n",
        "\n",
        "test_predictions = dc(dummies[:, 0])\n",
        "test_predictions"
      ],
      "metadata": {
        "id": "F1rweSb0wCfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the unscaled values of the outputs of the test set\n",
        "dummies = np.zeros((X_test.shape[0], TIMESTEP+1))\n",
        "dummies[:, 0] = y_test.flatten()\n",
        "dummies = scaler.inverse_transform(dummies)\n",
        "\n",
        "new_y_test = dc(dummies[:, 0])\n",
        "new_y_test"
      ],
      "metadata": {
        "id": "gbrTCGTN3p_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the Results of test set\n",
        "plt.plot(new_y_test, label='Actual Close')\n",
        "plt.plot(test_predictions, label='Predicted Close')\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Close')\n",
        "plt.legend()\n",
        "plt.title(\"AAMRATECH\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qZKmFDND3ur-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Functionizing the training process"
      ],
      "metadata": {
        "id": "ln1j1QMbf4k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_time_series(df, company_name=None):\n",
        "  print(f\"FOR {company_name}\")\n",
        "  print(\"--------------------------\")\n",
        "  print(\"--------------------------\")\n",
        "  df = df[[\"Date\", \"Close\"]]\n",
        "  df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "  plt.plot(df[\"Date\"], df[\"Close\"])\n",
        "  plt.xlabel('Day')\n",
        "  plt.ylabel('Close')\n",
        "  plt.title(f\"{company_name}\")\n",
        "  plt.show()\n",
        "\n",
        "  shifted_df = prepare_dataframe_for_lstm(df, 7)\n",
        "  shifted_df_as_np = shifted_df.to_numpy()\n",
        "\n",
        "  scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "  shifted_df_as_np = scaler.fit_transform(shifted_df_as_np)\n",
        "\n",
        "  X = shifted_df_as_np[:, 1:]\n",
        "  y = shifted_df_as_np[:, 0]\n",
        "\n",
        "  X = dc(np.flip(X, axis=1))\n",
        "\n",
        "  split_index = int(len(X) * 0.90)\n",
        "  X_train = X[:split_index]\n",
        "  X_test = X[split_index:]\n",
        "\n",
        "  y_train = y[:split_index]\n",
        "  y_test = y[split_index:]\n",
        "\n",
        "  X_train = X_train.reshape((-1, 7, 1))\n",
        "  X_test = X_test.reshape((-1, 7, 1))\n",
        "\n",
        "  y_train = y_train.reshape((-1, 1))\n",
        "  y_test = y_test.reshape((-1, 1))\n",
        "\n",
        "  X_train = torch.tensor(X_train).float()\n",
        "  X_test = torch.tensor(X_test).float()\n",
        "  y_train = torch.tensor(y_train).float()\n",
        "  y_test = torch.tensor(y_test).float()\n",
        "\n",
        "  train_dataset = TimeSeriesData(X_train, y_train)\n",
        "  test_dataset = TimeSeriesData(X_test, y_test)\n",
        "\n",
        "  batch_size = 32\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
        "\n",
        "  epochs = 200\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_step(model, train_dataloader, loss_fn, optimizer, epoch=epoch)\n",
        "    test_step(model, test_dataloader, loss_fn, epoch=epoch)\n",
        "\n",
        "  test_predictions = model(X_test.to(device)).detach().cpu().numpy().flatten()\n",
        "\n",
        "  dummies = np.zeros((X_test.shape[0], 7+1))\n",
        "  dummies[:, 0] = test_predictions\n",
        "  dummies = scaler.inverse_transform(dummies)\n",
        "\n",
        "  test_predictions = dc(dummies[:, 0])\n",
        "  test_predictions\n",
        "\n",
        "  dummies = np.zeros((X_test.shape[0], 7+1))\n",
        "  dummies[:, 0] = y_test.flatten()\n",
        "  dummies = scaler.inverse_transform(dummies)\n",
        "\n",
        "  new_y_test = dc(dummies[:, 0])\n",
        "  new_y_test\n",
        "\n",
        "  plt.plot(new_y_test, label='Actual Close')\n",
        "  plt.plot(test_predictions, label='Predicted Close')\n",
        "  plt.xlabel('Day')\n",
        "  plt.ylabel('Close')\n",
        "  if company_name:\n",
        "    plt.title(f\"{company_name}\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "b7fRmfQLAzY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. ADVENT"
      ],
      "metadata": {
        "id": "W0tmjNtYgc98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_advent = pd.read_csv(\"/content/data/stock_prices_data/timeseries_research_data/ADVENT_data.csv\")\n",
        "lstm_time_series(df_advent, \"ADVENT\")"
      ],
      "metadata": {
        "id": "X7cUAQhYBXVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. AGRODENIM"
      ],
      "metadata": {
        "id": "cgfRsK8kg8VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_agrodenim = pd.read_csv(\"/content/data/stock_prices_data/timeseries_research_data/ARGONDENIM_data.csv\")\n",
        "lstm_time_series(df_agrodenim, \"AGRODENIM\")"
      ],
      "metadata": {
        "id": "GhexKkTEXJ9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. BDTHAIFOOD"
      ],
      "metadata": {
        "id": "TxchTWOvieGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bdthaifood = pd.read_csv(\"/content/data/stock_prices_data/timeseries_research_data/BDTHAIFOOD_data.csv\")\n",
        "lstm_time_series(df_bdthaifood, \"BDTHAIFOOD\")"
      ],
      "metadata": {
        "id": "QTTnhYuzinTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. ECABLES"
      ],
      "metadata": {
        "id": "BYu-2s74ivfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ecables = pd.read_csv(\"/content/data/stock_prices_data/timeseries_research_data/ECABLES_data.csv\")\n",
        "lstm_time_series(df_ecables, \"ECABLES\")"
      ],
      "metadata": {
        "id": "ET_JgdmKi0cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. KEYACOSMET"
      ],
      "metadata": {
        "id": "tpDUAi9fjDlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_keyacosmet = pd.read_csv(\"/content/data/stock_prices_data/timeseries_research_data/KEYACOSMET_data.csv\")\n",
        "lstm_time_series(df_keyacosmet, \"KEYACOSMET\")"
      ],
      "metadata": {
        "id": "wW1TweMnjCvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. MEGHNAPET"
      ],
      "metadata": {
        "id": "hx5nV1HWjVHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_meghnapet = pd.read_csv(\"/content/data/stock_prices_data/timeseries_research_data/MEGHNAPET_data.csv\")\n",
        "lstm_time_series(df_meghnapet, \"MEGHNAPET\")"
      ],
      "metadata": {
        "id": "osUEuPY4jYWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. PRIMETEX"
      ],
      "metadata": {
        "id": "VVb4IL8zjiGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_primetex = pd.read_csv(\"/content/data/stock_prices_data/timeseries_research_data/PRIMETEX_data.csv\")\n",
        "lstm_time_series(df_primetex, \"PRIMETEX\")"
      ],
      "metadata": {
        "id": "3Ig83k_yjlj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. PUBALIBANK"
      ],
      "metadata": {
        "id": "_XinT4C8jwi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pubalibank = pd.read_csv(\"/content/data/stock_prices_data/timeseries_research_data/PUBALIBANK_data.csv\")\n",
        "lstm_time_series(df_pubalibank, \"PUBALIBANK\")"
      ],
      "metadata": {
        "id": "mmemoA6cjzc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. UNIONCAP"
      ],
      "metadata": {
        "id": "0tzhY3WUj5xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_unioncap = pd.read_csv(\"/content/data/stock_prices_data/timeseries_research_data/UNIONCAP_data.csv\")\n",
        "lstm_time_series(df_unioncap, \"UNIONCAP\")"
      ],
      "metadata": {
        "id": "81Mz_4uRj8mT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}